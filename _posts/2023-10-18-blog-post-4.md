---
title: "Summary of Fully Convolutional Networks for Semantic Segmentation"
excerpt: " 
The paper discusses Fully Convolutional Networks for Semantic Segmentation by Jonathan Long, Evan Shelhamer, and Trevor Darrell from UC Berkeley. The authors introduce fully convolutional networks (FCNs) as a powerful approach for semantic segmentation, surpassing existing methods in pixel-wise prediction. The key innovation is to replace fully-connected layers with convolutional layers, allowing for end-to-end training and efficient inference on arbitrary-sized inputs. The FCN architecture incorporates both deep, coarse semantic information and shallow, fine appearance information for accurate segmentations. The paper reviews related work, emphasizes the efficiency of fully convolutional training over patchwise methods, and demonstrates state-of-the-art results on various datasets. The authors compare their approach to adaptations of deep classification nets for semantic segmentation, highlighting the end-to-end learning capability of FCNs.
<br/><img src='/images/FCN.png'>"
---


The paper discusses "Fully Convolutional Networks for Semantic Segmentation" by Jonathan Long, Evan Shelhamer, and Trevor Darrell from UC Berkeley. The authors introduce fully convolutional networks (FCNs) as a powerful approach for semantic segmentation, surpassing existing methods in pixel-wise prediction. The key innovation is to replace fully-connected layers with convolutional layers, allowing for end-to-end training and efficient inference on arbitrary-sized inputs. The FCN architecture incorporates both deep, coarse semantic information and shallow, fine appearance information for accurate segmentations. The paper reviews related work, emphasizes the efficiency of fully convolutional training over patchwise methods, and demonstrates state-of-the-art results on various datasets. The authors compare their approach to adaptations of deep classification nets for semantic segmentation, highlighting the end-to-end learning capability of FCNs.

**Patchwise Method**

Patchwise training in computer vision involves training a model on smaller patches or sub-regions of an image rather than the entire image. Instead of feeding the entire image into the model during training, the image is divided into patches, and the model is trained on these smaller patches.

This approach has several advantages. It allows the model to focus on local features and details within an image, which can be particularly useful for tasks where local information is crucial, such as object recognition or segmentation. Patchwise training also enables the model to be more robust to variations within an image, as it learns to recognize patterns at a more granular level.

Additionally, patchwise training can help address challenges related to limited computational resources. Training on smaller patches reduces the input size, making it more feasible to train models on large datasets or high-resolution images.


## 1. Fully convolutional networks
Certainly! Here's the passage formatted in Markdown:

The passage discusses the architecture of Fully Convolutional Networks (FCNs) in the context of convolutional neural networks (convnets). Each layer in a convnet is represented as a three-dimensional array:

$$  h \times w \times d  $$


Here, *h* and *w* are spatial dimensions, and *d* is the feature or channel dimension. The first layer represents the image, with a pixel size of *h \times w* and *d* color channels. Locations in higher layers correspond to the locations in the image they are path-connected to, referred to as their receptive fields.

Convolutional operations in FCNs are defined as:

$$ y_{i,j} = f_{ks} \left(\{x_{si+\delta i, sj+\delta j} \mid 0 \leq \delta i, \delta j \leq k\}\right) $$

where:
- *k* is the kernel size,
- *s* is the stride or subsampling factor,
- $f_{ks}$  determines the layer type (e.g., matrix multiplication for convolution or average pooling, spatial max for max pooling, etc.).

The passage emphasizes the importance of this functional form being maintained under composition. A net with only layers of this form is referred to as a fully convolutional network (FCN), capable of operating on inputs of any size and producing outputs with corresponding spatial dimensions.

The real-valued loss function $\( \mathcal{L} \)$ composed with an FCN is defined as:

$$ \mathcal{L}(x; \theta) = \sum_{i,j} \ell \left(x_{i,j}; \theta\right) $$



where *l* is the loss function for a spatial component. The gradient of $\( \mathcal{L} \)$ is a sum over the gradients of each spatial component.

Efficiency is highlighted when feedforward computation and backpropagation are computed layer-by-layer over an entire image, particularly when receptive fields overlap significantly:


$$ f_{k,s} \circ g_{k',s'} = (f \circ g)_{k'+(k-1)s', ss'} $$


### 1.1 Adapting classifiers for dense prediction

<img src="/images/FCN2.png" alt="alt text" height="400" />

Figure 2, Transforming fully connected layers into convolution layers enables a classification net to output a heatmap.

The passage discusses the adaptation of typical recognition networks, such as LeNet, AlexNet, and their successors, for dense prediction tasks. The fully connected layers in these networks, originally designed for fixed-sized inputs, can be viewed as convolutions covering their entire input regions. This transformation turns them into fully convolutional networks capable of taking inputs of any size and producing classification maps.

The illustration in Figure 2 depicts the convolutionalization process, showing how fully connected layers can output heatmaps. This adaptation allows classification nets to efficiently produce dense predictions. The computational efficiency is highlighted by comparing the processing times of the original net and its fully convolutional version.

The spatial output maps generated by convolutionalized models make them suitable for dense problems like semantic segmentation. The availability of ground truth at every output cell facilitates both forward and backward passes, taking advantage of the inherent computational efficiency of convolutions.

Although the reinterpretation of classification nets as fully convolutional enables output maps for inputs of any size, the output dimensions are typically reduced through subsampling. This subsampling is employed to keep filters small and manage computational requirements, resulting in a coarser output compared to the input size.

### 1.2 Shift-and-stitch is filter rarefaction
This section discusses the "shift-and-stitch" technique, a method to obtain dense predictions without interpolation. It involves shifting the input for each value of (x, y) in a specific range. While it achieves denser outputs, it comes with trade-offs, including longer computation times.

To replicate shift-and-stitch in a convnet, filters, and layer strides are modified. The passage outlines the rarefaction process but notes its trade-offs. Decreasing subsampling allows filters to see finer information but with smaller receptive fields and longer computation. Shift-and-stitch densifies output without reducing receptive fields but limits filter access to finer details.

Despite preliminary experiments, the authors prefer learning through upsampling, finding it more effective and efficient, especially when combined with skip layer fusion in their model.

### 1.3 Upsampling is backwards strided convolution
This section discusses the concept of upsampling in the context of connecting coarse outputs to dense pixels. Upsampling is likened to backwards strided convolution, where the fractional input stride of 1/f is achieved through convolution with a fractional output stride of f. When f is integral, upsampling can be effectively performed using backwards convolution (or deconvolution), which reverses the forward and backward passes of convolution.

Upsampling is considered a crucial step in end-to-end learning, allowing for efficient backpropagation from pixelwise loss. The deconvolution filter used in such a layer can be flexible and learned rather than fixed, enabling the network to adapt and potentially learn nonlinear upsampling. The passage emphasizes that in-network upsampling, particularly through learned deconvolution layers, proves to be fast and effective in the experiments conducted.
