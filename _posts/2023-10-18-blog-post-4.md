---
title: "Summary of Fully Convolutional Networks for Semantic Segmentation"
excerpt: " 
The paper discusses Fully Convolutional Networks for Semantic Segmentation by Jonathan Long, Evan Shelhamer, and Trevor Darrell from UC Berkeley. The authors introduce fully convolutional networks (FCNs) as a powerful approach for semantic segmentation, surpassing existing methods in pixel-wise prediction. The key innovation is to replace fully-connected layers with convolutional layers, allowing for end-to-end training and efficient inference on arbitrary-sized inputs. The FCN architecture incorporates both deep, coarse semantic information and shallow, fine appearance information for accurate segmentations. The paper reviews related work, emphasizes the efficiency of fully convolutional training over patchwise methods, and demonstrates state-of-the-art results on various datasets. The authors compare their approach to adaptations of deep classification nets for semantic segmentation, highlighting the end-to-end learning capability of FCNs.
<br/><img src='/images/FCN.png'>"
---


The paper discusses "Fully Convolutional Networks for Semantic Segmentation" by Jonathan Long, Evan Shelhamer, and Trevor Darrell from UC Berkeley. The authors introduce fully convolutional networks (FCNs) as a powerful approach for semantic segmentation, surpassing existing methods in pixel-wise prediction. The key innovation is to replace fully-connected layers with convolutional layers, allowing for end-to-end training and efficient inference on arbitrary-sized inputs. The FCN architecture incorporates both deep, coarse semantic information and shallow, fine appearance information for accurate segmentations. The paper reviews related work, emphasizes the efficiency of fully convolutional training over patchwise methods, and demonstrates state-of-the-art results on various datasets. The authors compare their approach to adaptations of deep classification nets for semantic segmentation, highlighting the end-to-end learning capability of FCNs.


## 1. Fully convolutional networks
The paper discusses the architecture of Fully Convolutional Networks (FCNs) in the context of convolutional neural networks (convnets). Each layer in a convnet is represented as a three-dimensional array:

$$  h \times w \times d  $$


Here, *h* and *w* are spatial dimensions, and *d* is the feature or channel dimension. The first layer represents the image, with a pixel size of *h \times w* and *d* color channels. Locations in higher layers correspond to the locations in the image they are path-connected to, referred to as their receptive fields.

Convolutional operations in FCNs are defined as:

$$ y_{i,j} = f_{ks} \left(\{x_{si+\delta i, sj+\delta j} \mid 0 \leq \delta i, \delta j \leq k\}\right) $$

where:
- *k* is the kernel size,
- *s* is the stride or subsampling factor,
- $f_{ks}$  determines the layer type (e.g., matrix multiplication for convolution or average pooling, spatial max for max pooling, etc.).

The passage emphasizes the importance of this functional form being maintained under composition. A net with only layers of this form is referred to as a fully convolutional network (FCN), capable of operating on inputs of any size and producing outputs with corresponding spatial dimensions.

The real-valued loss function $\( \mathcal{L} \)$ composed with an FCN is defined as:

$$ \mathcal{L}(x; \theta) = \sum_{i,j} \ell \left(x_{i,j}; \theta\right) $$



where *l* is the loss function for a spatial component. The gradient of $\( \mathcal{L} \)$ is a sum over the gradients of each spatial component.

Efficiency is highlighted when feedforward computation and backpropagation are computed layer-by-layer over an entire image, particularly when receptive fields overlap significantly:


$$ f_{k,s} \circ g_{k',s'} = (f \circ g)_{k'+(k-1)s', ss'} $$


### 1.1 Adapting classifiers for dense prediction

<img src="/images/FCN2.png" alt="alt text" height="400" />

Figure 2, Transforming fully connected layers into convolution layers enables a classification net to output a heatmap.

In section 3.1, the paper discusses the adaptation of typical recognition networks, such as LeNet, AlexNet, and their successors, for dense prediction tasks. The fully connected layers in these networks, originally designed for fixed-sized inputs, can be viewed as convolutions covering their entire input regions. This transformation turns them into fully convolutional networks capable of taking inputs of any size and producing classification maps.

The illustration in Figure 2 depicts the convolutionalization process, showing how fully connected layers can output heatmaps. This adaptation allows classification nets to efficiently produce dense predictions. The computational efficiency is highlighted by comparing the processing times of the original net and its fully convolutional version.

The spatial output maps generated by convolutionalized models make them suitable for dense problems like semantic segmentation. The availability of ground truth at every output cell facilitates both forward and backward passes, taking advantage of the inherent computational efficiency of convolutions.

Although the reinterpretation of classification nets as fully convolutional enables output maps for inputs of any size, the output dimensions are typically reduced through subsampling. This subsampling is employed to keep filters small and manage computational requirements, resulting in a coarser output compared to the input size.

### 1.2 Shift-and-stitch is filter rarefaction
In section 3.2, the paper discusses the "shift-and-stitch" technique, a method to obtain dense predictions without interpolation. It involves shifting the input for each value of (x, y) in a specific range. While it achieves denser outputs, it comes with trade-offs, including longer computation times.

To replicate shift-and-stitch in a convnet, filters, and layer strides are modified. The passage outlines the rarefaction process but notes its trade-offs. Decreasing subsampling allows filters to see finer information but with smaller receptive fields and longer computation. Shift-and-stitch densifies output without reducing receptive fields but limits filter access to finer details.

Despite preliminary experiments, the authors prefer learning through upsampling, finding it more effective and efficient, especially when combined with skip layer fusion in their model.

### 1.3 Upsampling is backwards strided convolution
In section 3.3, the paper discusses the concept of upsampling in the context of connecting coarse outputs to dense pixels. Upsampling is likened to backwards strided convolution, where the fractional input stride of 1/f is achieved through convolution with a fractional output stride of f. When f is integral, upsampling can be effectively performed using backwards convolution (or deconvolution), which reverses the forward and backward passes of convolution.

Upsampling is considered a crucial step in end-to-end learning, allowing for efficient backpropagation from pixelwise loss. The deconvolution filter used in such a layer can be flexible and learned rather than fixed, enabling the network to adapt and potentially learn nonlinear upsampling. The passage emphasizes that in-network upsampling, particularly through learned deconvolution layers, proves to be fast and effective in the experiments conducted.

### 1.4 Patchwise training is loss sampling
In section 3.4, the paper discusses patchwise training as a form of loss sampling in stochastic optimization. Both patchwise and fully-convolutional training can produce various distributions, with computational efficiency influenced by factors like overlap and minibatch size.

Whole image fully convolutional training is akin to patchwise training, but it includes all receptive fields in each batch, increasing efficiency. However, random patch selection within an image can restore diversity.

The paper notes that excluding patches from gradient computation, achieved through loss sampling or a DropConnect mask, helps correct class imbalance and address spatial correlation in dense patches. Despite exploring sampling in Section 4.3, the paper finds that it doesn't yield faster or better convergence for dense prediction. Whole image training remains effective and efficient.


### Additional Knowledge

**Patchwise Method**

Patchwise training in computer vision involves training a model on smaller patches or sub-regions of an image rather than the entire image. Instead of feeding the entire image into the model during training, the image is divided into patches, and the model is trained on these smaller patches.

This approach has several advantages. It allows the model to focus on local features and details within an image, which can be particularly useful for tasks where local information is crucial, such as object recognition or segmentation. Patchwise training also enables the model to be more robust to variations within an image, as it learns to recognize patterns at a more granular level.

Additionally, patchwise training can help address challenges related to limited computational resources. Training on smaller patches reduces the input size, making it more feasible to train models on large datasets or high-resolution images.

Pre- and post-processing complications in the context of computer vision refer to additional steps or techniques applied before or after the main processing stages of a computer vision algorithm. Let's break down the mentioned components:

1. **Pre-processing:**
    - **Superpixels:** Superpixels are compact and perceptually meaningful atomic regions obtained by grouping similar pixels together. They can be used as a pre-processing step to reduce the complexity of an image, making subsequent processing more efficient. Superpixels can enhance boundary information and improve segmentation results.
    - **Proposals:** Object proposals are candidate bounding boxes that potentially contain objects of interest. They are generated as a pre-processing step to reduce the search space for object detection tasks. Instead of exhaustively examining all possible locations, the algorithm focuses on a smaller set of proposals, improving efficiency.
  
2. **Post-processing:**
    - **Random Fields:** Random fields, particularly Markov Random Fields (MRFs), are used for post-hoc refinement in segmentation and classification tasks. They model spatial dependencies between pixels or regions and can improve the coherence of the final result. Random fields are often employed to enforce smoothness constraints in the output.
    - **Local Classifiers:** Post-hoc refinement using local classifiers involves using additional classifiers to fine-tune or correct the predictions made by the main model. This is often done at a more granular level, addressing specific regions or instances where the primary model may have made errors.

These complications are introduced to enhance the performance, robustness, and efficiency of computer vision algorithms. Superpixels and proposals help in handling complex images more effectively, while post-processing techniques like random fields and local classifiers aim to improve the precision and coherence of the final output. The goal is to address challenges such as noise, inaccuracies, or lack of global context in the initial processing stages.


The statement highlights a fundamental challenge in semantic segmentation, which is the task of assigning semantic labels to each pixel in an image. The tension referred to is between understanding the semantics (meaning or category) of objects in the image and precisely locating where these objects are.

1. **Semantics vs. Location:**
   - **Semantics:** This refers to the understanding of what objects or entities are present in the image. For example, recognizing that there are people, cars, or buildings.
   - **Location:** This involves determining where these objects are located in the image, the spatial information.

2. **Global Information Resolves What:**
   - Global information looks at the entire context of the image. Understanding the overall scene and the types of objects present falls under global information. This helps in resolving "what" is in the image at a broader level.

3. **Local Information Resolves Where:**
   - Local information, on the other hand, focuses on specific regions or details within the image. It helps in pinpointing "where" exactly in the image certain objects or features are located.

4. **Deep Feature Hierarchies:**
   - Deep feature hierarchies are a part of deep learning architectures. These hierarchies consist of layers that progressively learn more abstract and complex features. In this context, these hierarchies are jointly encoding both the location and semantics.
   
5. **Local-to-Global Pyramid:**
   - The local-to-global pyramid suggests that the model or algorithm incorporates information at different scales, ranging from local details to global context. This pyramid structure ensures that the model considers information at various levels of granularity.

In summary, the challenge in semantic segmentation is to strike a balance between understanding what is in the image (semantics) and precisely locating where these elements are situated. Deep feature hierarchies, organized in a local-to-global pyramid, are proposed to address this tension by jointly encoding both semantic and spatial information.
